{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T12:07:18.960207Z",
     "start_time": "2025-05-16T11:54:05.578399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pyttsx3  # Text-to-speech library\n",
    "import threading  # For non-blocking speech\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "tts_engine = pyttsx3.init()\n",
    "tts_engine.setProperty('rate', 150)  # Speed of speech (words per minute)\n",
    "tts_engine.setProperty('volume', 1.0)  # Volume level (0.0 to 1.0)\n",
    "\n",
    "# Global variables for tracking and triggers\n",
    "object_tracker = defaultdict(lambda: {'count': 0, 'last_seen': 0})\n",
    "trigger_cooldown = defaultdict(float)\n",
    "last_detection_time = 0\n",
    "description_cooldown = 5  # seconds between descriptions for the same object type\n",
    "last_spoken = \"\"  # Track last spoken phrase to avoid repeats\n",
    "\n",
    "# Object descriptions database\n",
    "OBJECT_DESCRIPTIONS = {\n",
    "    'backpack': \"A backpack is a bag carried on one's back, typically made of cloth or leather with straps over the shoulders.\",\n",
    "    'bottle-a': \"A bottle is a container with a neck that is narrower than the body, used for storing liquids.\",\n",
    "    'bottle-b': \"A second type of bottle, possibly different in shape or size from bottle-a.\",\n",
    "    'bowl': \"A bowl is a round, deep dish used for preparing or serving food.\",\n",
    "    'casserole': \"A casserole is a large, deep dish used both in the oven and as a serving vessel.\",\n",
    "    'chair': \"A chair is a piece of furniture designed for sitting, typically with four legs and a back.\",\n",
    "    'cup': \"A cup is a small open container used for drinking, usually with a handle.\",\n",
    "    'fork': \"A fork is a utensil with prongs used for eating or serving food.\",\n",
    "    'frigo': \"A refrigerator is a cooling appliance used to preserve food at low temperatures.\",\n",
    "    'glass': \"A glass is a hard, brittle substance made by fusing sand with soda and lime, used for drinking containers.\",\n",
    "    'handbag': \"A handbag is a small bag used to carry personal items, typically carried by women.\",\n",
    "    'iphone': \"An iPhone is a line of smartphones designed and marketed by Apple Inc.\",\n",
    "    'knife': \"A knife is a tool with a cutting edge or blade, used for cutting or as a weapon.\",\n",
    "    'lamp': \"A lamp is a device that produces light, typically using electricity or oil.\",\n",
    "    'laptop': \"A laptop is a portable computer suitable for use while traveling.\",\n",
    "    'macbook': \"A MacBook is a brand of Macintosh laptop computers by Apple Inc.\",\n",
    "    'micro-ondes': \"A microwave is an electric oven that heats and cooks food by exposing it to electromagnetic radiation.\",\n",
    "    'oldphone': \"An old phone refers to earlier models of telephones, possibly rotary or early mobile phones.\",\n",
    "    'paperbag': \"A paper bag is a bag made of paper, usually used for carrying goods.\",\n",
    "    'plate': \"A plate is a flat dish for holding food, typically circular and made of china or other materials.\",\n",
    "    'smartphone': \"A smartphone is a mobile phone with advanced features beyond basic calling, typically with internet access.\",\n",
    "    'sofa': \"A sofa is a long upholstered seat with a back and arms, for two or more people.\",\n",
    "    'spoon': \"A spoon is a utensil with a shallow bowl on a handle, used for eating, stirring, and serving food.\",\n",
    "    'table': \"A table is a piece of furniture with a flat top and one or more legs, used as a surface for working or eating.\",\n",
    "    'washmachine': \"A washing machine is a machine for washing laundry, such as clothing and sheets.\"\n",
    "}\n",
    "\n",
    "def speak(text, priority=1):\n",
    "    \"\"\"Speak text using TTS (runs in a separate thread to avoid blocking)\"\"\"\n",
    "    global last_spoken\n",
    "\n",
    "    # Don't repeat the same phrase or interrupt higher priority messages\n",
    "    if text == last_spoken or (tts_engine.isBusy() and priority < 2):\n",
    "        return\n",
    "\n",
    "    last_spoken = text\n",
    "    threading.Thread(target=tts_engine.say, args=(text,)).start()\n",
    "    try:\n",
    "        # This will run in the thread\n",
    "        tts_engine.runAndWait()\n",
    "    except RuntimeError:  # Ignore errors from thread management\n",
    "        pass\n",
    "\n",
    "def get_object_description(class_name):\n",
    "    \"\"\"Returns a detailed description of the detected object\"\"\"\n",
    "    return OBJECT_DESCRIPTIONS.get(class_name, f\"A {class_name} was detected.\")\n",
    "\n",
    "def trigger_action(class_name, confidence, box_xyxy, frame_shape):\n",
    "    \"\"\"\n",
    "    Enhanced function to trigger actions and provide descriptions when objects are detected.\n",
    "    Now includes text-to-speech output.\n",
    "    \"\"\"\n",
    "    global last_detection_time\n",
    "\n",
    "    current_time = time.time()\n",
    "    box_area = (box_xyxy[2] - box_xyxy[0]) * (box_xyxy[3] - box_xyxy[1])\n",
    "    frame_area = frame_shape[0] * frame_shape[1]\n",
    "    relative_size = box_area / frame_area\n",
    "\n",
    "    # Update object tracking\n",
    "    object_tracker[class_name]['count'] += 1\n",
    "    object_tracker[class_name]['last_seen'] = current_time\n",
    "\n",
    "    # Basic detection info (printed to console)\n",
    "    detection_msg = f\"{class_name} detected with {confidence:.0%} confidence\"\n",
    "    print(f\"\\n{detection_msg}\")\n",
    "    speak(detection_msg, priority=1)\n",
    "\n",
    "    # Position analysis\n",
    "    center_x = (box_xyxy[0] + box_xyxy[2]) // 2\n",
    "    center_y = (box_xyxy[1] + box_xyxy[3]) // 2\n",
    "\n",
    "    # Determine position in frame\n",
    "    x_pos = \"left\" if center_x < frame_shape[1] // 3 else \"right\" if center_x > 2 * frame_shape[1] // 3 else \"center\"\n",
    "    y_pos = \"top\" if center_y < frame_shape[0] // 3 else \"bottom\" if center_y > 2 * frame_shape[0] // 3 else \"middle\"\n",
    "    position = f\"{x_pos} {y_pos}\"\n",
    "\n",
    "    print(f\"  Position: {position}, Size: {relative_size:.1%}\")\n",
    "\n",
    "    # Speak position if object is large enough\n",
    "    if relative_size > 0.1:  # Only announce position for significant objects\n",
    "        speak(f\"located at {x_pos} {y_pos}\", priority=1)\n",
    "\n",
    "    # Display object description (with cooldown)\n",
    "    if current_time - trigger_cooldown.get(class_name, 0) > description_cooldown:\n",
    "        description = get_object_description(class_name)\n",
    "        print(f\"\\nAI DESCRIPTION: {description}\")\n",
    "        speak(description, priority=2)\n",
    "        trigger_cooldown[class_name] = current_time\n",
    "\n",
    "    # Special triggers with TTS alerts\n",
    "    trigger_cooldown_time = 10  # seconds between allowed special triggers\n",
    "\n",
    "    # Kitchen safety triggers\n",
    "    if class_name == 'knife' and confidence > 0.75:\n",
    "        if current_time - trigger_cooldown.get('knife_safety', 0) > trigger_cooldown_time:\n",
    "            alert = \"Safety warning! Knife detected. Please handle with care.\"\n",
    "            print(f\"\\nSAFETY NOTICE: {alert}\")\n",
    "            speak(alert, priority=3)  # Highest priority\n",
    "            trigger_cooldown['knife_safety'] = current_time\n",
    "\n",
    "    # Appliance triggers\n",
    "    elif class_name == 'frigo' and confidence > 0.7:\n",
    "        if current_time - trigger_cooldown.get('frigo_check', 0) > trigger_cooldown_time:\n",
    "            alert = \"Refrigerator detected. Optimal temperature is zero to four degrees Celsius.\"\n",
    "            if relative_size > 0.3:\n",
    "                alert += \" Warning: The door appears to be open.\"\n",
    "            print(f\"\\nAPPLIANCE NOTICE: {alert}\")\n",
    "            speak(alert, priority=3)\n",
    "            trigger_cooldown['frigo_check'] = current_time\n",
    "\n",
    "    # Electronic devices\n",
    "    elif class_name in ['laptop', 'macbook'] and confidence > 0.8:\n",
    "        if current_time - trigger_cooldown.get('laptop_notice', 0) > trigger_cooldown_time:\n",
    "            alert = \"Laptop detected. Remember to take regular breaks from screen time.\"\n",
    "            print(f\"\\nDEVICE NOTICE: {alert}\")\n",
    "            speak(alert, priority=2)\n",
    "            trigger_cooldown['laptop_notice'] = current_time\n",
    "\n",
    "    # Phone detection\n",
    "    elif class_name in ['iphone', 'smartphone'] and confidence > 0.85:\n",
    "        if current_time - trigger_cooldown.get('phone_notice', 0) > trigger_cooldown_time:\n",
    "            alert = \"Smartphone detected. Consider putting it down if not needed.\"\n",
    "            print(f\"\\nDEVICE NOTICE: {alert}\")\n",
    "            speak(alert, priority=2)\n",
    "            trigger_cooldown['phone_notice'] = current_time\n",
    "\n",
    "    # Update last detection time\n",
    "    last_detection_time = current_time\n",
    "\n",
    "def cleanup_old_objects():\n",
    "    \"\"\"Clean up objects not seen in the last 30 seconds from tracker\"\"\"\n",
    "    current_time = time.time()\n",
    "    to_delete = [name for name, data in object_tracker.items()\n",
    "                 if current_time - data['last_seen'] > 30]\n",
    "    for name in to_delete:\n",
    "        del object_tracker[name]\n",
    "\n",
    "def print_detection_summary():\n",
    "    \"\"\"Print summary of detected objects and speak if new items\"\"\"\n",
    "    if object_tracker:\n",
    "        summary = \"\\nDETECTION SUMMARY:\"\n",
    "        print(summary)\n",
    "\n",
    "        # Build spoken summary for important items\n",
    "        spoken_items = []\n",
    "        for obj, data in object_tracker.items():\n",
    "            line = f\"  - {obj}: detected {data['count']} times\"\n",
    "            print(line)\n",
    "\n",
    "            # Only speak items detected recently and more than once\n",
    "            if time.time() - data['last_seen'] < 10 and data['count'] > 1:\n",
    "                spoken_items.append(f\"{data['count']} {obj}s\")\n",
    "\n",
    "        if spoken_items:\n",
    "            speak(\"Current items: \" + \", \".join(spoken_items), priority=2)\n",
    "    else:\n",
    "        print(\"\\nNo objects detected yet.\")\n",
    "\n",
    "def run_custom_inference():\n",
    "    \"\"\"\n",
    "    Runs inference using a custom-trained YOLOv8 model with:\n",
    "    - Text-to-speech descriptions\n",
    "    - AI triggers with vocal alerts\n",
    "    - Real-time object tracking\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    model_path_str = 'yolo_room_detection_result/cpu_run12/weights/best.pt'\n",
    "    source_input = '0'  # Default webcam\n",
    "\n",
    "    confidence_threshold = 0.5\n",
    "    iou_threshold = 0.45\n",
    "\n",
    "    # Load Model\n",
    "    model_path = Path(model_path_str).resolve()\n",
    "    if not model_path.exists():\n",
    "        error_msg = f\"Error: Model weights file not found at {model_path}\"\n",
    "        print(error_msg)\n",
    "        speak(error_msg, priority=3)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "        load_msg = f\"Loaded custom YOLO model with {len(model.names)} classes\"\n",
    "        print(load_msg)\n",
    "        speak(load_msg, priority=1)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error loading YOLO model: {e}\"\n",
    "        print(error_msg)\n",
    "        speak(error_msg, priority=3)\n",
    "        return\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Source Handling\n",
    "    is_webcam = False\n",
    "    is_image = False\n",
    "    is_video_or_stream = False\n",
    "\n",
    "    if isinstance(source_input, str) and source_input.isdigit():\n",
    "        processed_source = int(source_input)\n",
    "        is_webcam = True\n",
    "    elif isinstance(source_input, int):\n",
    "        processed_source = source_input\n",
    "        is_webcam = True\n",
    "    elif isinstance(source_input, str):\n",
    "        source_path = Path(source_input)\n",
    "        if source_path.suffix.lower() in ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff', '.webp']:\n",
    "            if not source_path.exists():\n",
    "                error_msg = f\"Image file not found at {source_path}\"\n",
    "                print(error_msg)\n",
    "                speak(error_msg, priority=3)\n",
    "                return\n",
    "            processed_source = str(source_path.resolve())\n",
    "            is_image = True\n",
    "        elif source_path.suffix.lower() in ['.mp4', '.avi', '.mov', '.mkv'] or \\\n",
    "             source_input.lower().startswith((\"rtsp://\", \"http://\", \"https://\")):\n",
    "            if not (source_input.lower().startswith((\"rtsp://\", \"http://\", \"https://\")) or source_path.exists()):\n",
    "                error_msg = f\"Video file not found at {source_path}\"\n",
    "                print(error_msg)\n",
    "                speak(error_msg, priority=3)\n",
    "                return\n",
    "            processed_source = source_input\n",
    "            is_video_or_stream = True\n",
    "        else:\n",
    "            error_msg = f\"Unsupported file type: {source_input}\"\n",
    "            print(error_msg)\n",
    "            speak(error_msg, priority=3)\n",
    "            return\n",
    "    else:\n",
    "        error_msg = f\"Invalid source type: {source_input}\"\n",
    "        print(error_msg)\n",
    "        speak(error_msg, priority=3)\n",
    "        return\n",
    "\n",
    "    # Processing\n",
    "    if is_image:\n",
    "        print(f\"Processing image: {processed_source}\")\n",
    "        try:\n",
    "            results = model(processed_source, conf=confidence_threshold, iou=iou_threshold, device=device)\n",
    "\n",
    "            if results and results[0]:\n",
    "                annotated_frame = results[0].plot()\n",
    "                img_shape = results[0].orig_shape\n",
    "\n",
    "                for box in results[0].boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    conf = float(box.conf[0])\n",
    "                    cls_id = int(box.cls[0])\n",
    "                    class_name = model.names[cls_id] if model.names and cls_id < len(model.names) else f\"ClassID:{cls_id}\"\n",
    "                    trigger_action(class_name, conf, [x1, y1, x2, y2], img_shape)\n",
    "\n",
    "                print_detection_summary()\n",
    "                cv2.imshow(\"YOLOv8 Detection - Image\", annotated_frame)\n",
    "                cv2.waitKey(0)\n",
    "            else:\n",
    "                msg = \"No objects detected in the image\"\n",
    "                print(msg)\n",
    "                speak(msg, priority=1)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Image processing error: {e}\"\n",
    "            print(error_msg)\n",
    "            speak(error_msg, priority=3)\n",
    "        finally:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    elif is_video_or_stream or is_webcam:\n",
    "        cap = None\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(processed_source)\n",
    "            if not cap.isOpened():\n",
    "                error_msg = f\"Could not open video source: {processed_source}\"\n",
    "                print(error_msg)\n",
    "                speak(error_msg, priority=3)\n",
    "                return\n",
    "\n",
    "            source_name = \"Webcam\" if is_webcam else \"Video\"\n",
    "            start_msg = f\"Starting {source_name} detection. Press Q to quit or S for summary.\"\n",
    "            print(start_msg)\n",
    "            speak(start_msg, priority=1)\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    end_msg = f\"{source_name} stream ended.\"\n",
    "                    print(end_msg)\n",
    "                    speak(end_msg, priority=1)\n",
    "                    if not is_webcam:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                # Clean up old objects periodically\n",
    "                if time.time() % 10 < 0.1:\n",
    "                    cleanup_old_objects()\n",
    "\n",
    "                # Perform detection\n",
    "                results = model(frame, conf=confidence_threshold, iou=iou_threshold, device=device, verbose=False)\n",
    "\n",
    "                if results and results[0]:\n",
    "                    annotated_frame = results[0].plot()\n",
    "                    frame_shape = results[0].orig_shape\n",
    "\n",
    "                    for box in results[0].boxes:\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        conf = float(box.conf[0])\n",
    "                        cls_id = int(box.cls[0])\n",
    "                        class_name = model.names[cls_id] if model.names and cls_id < len(model.names) else f\"ClassID:{cls_id}\"\n",
    "                        trigger_action(class_name, conf, [x1, y1, x2, y2], frame_shape)\n",
    "\n",
    "                    cv2.imshow(f\"YOLOv8 Detection - {source_name}\", annotated_frame)\n",
    "                else:\n",
    "                    cv2.imshow(f\"YOLOv8 Detection - {source_name}\", frame)\n",
    "\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    print_detection_summary()\n",
    "                    break_msg = \"Stopping object detection.\"\n",
    "                    print(break_msg)\n",
    "                    speak(break_msg, priority=1)\n",
    "                    break\n",
    "                elif key == ord('s'):\n",
    "                    print_detection_summary()\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Video processing error: {e}\"\n",
    "            print(error_msg)\n",
    "            speak(error_msg, priority=3)\n",
    "        finally:\n",
    "            if cap:\n",
    "                cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"Detection stopped.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    intro = \"\"\"\n",
    "    YOLOv8 Object Detection with Voice Assistant\n",
    "    Features:\n",
    "    - Real-time object detection\n",
    "    - Spoken descriptions of detected objects\n",
    "    - Safety alerts for dangerous items\n",
    "    - Device usage reminders\n",
    "    Press Q to quit, S for summary during detection.\n",
    "    \"\"\"\n",
    "    print(intro)\n",
    "    speak(\"Starting YOLO object detection with voice feedback.\", priority=1)\n",
    "    run_custom_inference()"
   ],
   "id": "a79561f28ceeeb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    YOLOv8 Object Detection with Voice Assistant\n",
      "    Features:\n",
      "    - Real-time object detection\n",
      "    - Spoken descriptions of detected objects\n",
      "    - Safety alerts for dangerous items\n",
      "    - Device usage reminders\n",
      "    Press Q to quit, S for summary during detection.\n",
      "    \n",
      "Loaded custom YOLO model with 25 classes\n",
      "Using device: cpu\n",
      "Starting Webcam detection. Press Q to quit or S for summary.\n",
      "\n",
      "macbook detected with 66% confidence\n",
      "  Position: center middle, Size: 99.7%\n",
      "\n",
      "AI DESCRIPTION: A MacBook is a brand of Macintosh laptop computers by Apple Inc.\n",
      "\n",
      "iphone detected with 72% confidence\n",
      "  Position: left middle, Size: 56.1%\n",
      "\n",
      "AI DESCRIPTION: An iPhone is a line of smartphones designed and marketed by Apple Inc.\n",
      "\n",
      "laptop detected with 57% confidence\n",
      "  Position: center middle, Size: 99.8%\n",
      "\n",
      "AI DESCRIPTION: A laptop is a portable computer suitable for use while traveling.\n",
      "\n",
      "chair detected with 51% confidence\n",
      "  Position: center middle, Size: 90.0%\n",
      "\n",
      "AI DESCRIPTION: A chair is a piece of furniture designed for sitting, typically with four legs and a back.\n",
      "\n",
      "laptop detected with 69% confidence\n",
      "  Position: center middle, Size: 99.7%\n",
      "\n",
      "AI DESCRIPTION: A laptop is a portable computer suitable for use while traveling.\n",
      "\n",
      "macbook detected with 60% confidence\n",
      "  Position: center middle, Size: 99.5%\n",
      "\n",
      "AI DESCRIPTION: A MacBook is a brand of Macintosh laptop computers by Apple Inc.\n",
      "\n",
      "macbook detected with 65% confidence\n",
      "  Position: center middle, Size: 90.0%\n",
      "\n",
      "AI DESCRIPTION: A MacBook is a brand of Macintosh laptop computers by Apple Inc.\n",
      "\n",
      "table detected with 53% confidence\n",
      "  Position: right bottom, Size: 18.0%\n",
      "\n",
      "AI DESCRIPTION: A table is a piece of furniture with a flat top and one or more legs, used as a surface for working or eating.\n",
      "\n",
      "macbook detected with 79% confidence\n",
      "  Position: center middle, Size: 99.1%\n",
      "\n",
      "AI DESCRIPTION: A MacBook is a brand of Macintosh laptop computers by Apple Inc.\n",
      "\n",
      "macbook detected with 74% confidence\n",
      "  Position: center middle, Size: 99.7%\n",
      "\n",
      "AI DESCRIPTION: A MacBook is a brand of Macintosh laptop computers by Apple Inc.\n",
      "\n",
      "laptop detected with 75% confidence\n",
      "  Position: center bottom, Size: 64.1%\n",
      "\n",
      "AI DESCRIPTION: A laptop is a portable computer suitable for use while traveling.\n",
      "\n",
      "sofa detected with 58% confidence\n",
      "  Position: center bottom, Size: 15.0%\n",
      "\n",
      "AI DESCRIPTION: A sofa is a long upholstered seat with a back and arms, for two or more people.\n",
      "\n",
      "frigo detected with 56% confidence\n",
      "  Position: center middle, Size: 99.5%\n",
      "\n",
      "AI DESCRIPTION: A refrigerator is a cooling appliance used to preserve food at low temperatures.\n",
      "\n",
      "laptop detected with 51% confidence\n",
      "  Position: center middle, Size: 97.1%\n",
      "\n",
      "AI DESCRIPTION: A laptop is a portable computer suitable for use while traveling.\n",
      "\n",
      "macbook detected with 56% confidence\n",
      "  Position: center middle, Size: 99.5%\n",
      "\n",
      "AI DESCRIPTION: A MacBook is a brand of Macintosh laptop computers by Apple Inc.\n",
      "\n",
      "macbook detected with 73% confidence\n",
      "  Position: center middle, Size: 98.4%\n",
      "\n",
      "AI DESCRIPTION: A MacBook is a brand of Macintosh laptop computers by Apple Inc.\n",
      "\n",
      "macbook detected with 84% confidence\n",
      "  Position: center middle, Size: 99.4%\n",
      "\n",
      "DEVICE NOTICE: Laptop detected. Remember to take regular breaks from screen time.\n",
      "\n",
      "macbook detected with 71% confidence\n",
      "  Position: center middle, Size: 97.3%\n",
      "\n",
      "AI DESCRIPTION: A MacBook is a brand of Macintosh laptop computers by Apple Inc.\n",
      "\n",
      "macbook detected with 58% confidence\n",
      "  Position: center middle, Size: 98.9%\n",
      "\n",
      "AI DESCRIPTION: A MacBook is a brand of Macintosh laptop computers by Apple Inc.\n",
      "\n",
      "macbook detected with 59% confidence\n",
      "  Position: center middle, Size: 99.4%\n",
      "\n",
      "macbook detected with 57% confidence\n",
      "  Position: center middle, Size: 99.4%\n",
      "\n",
      "table detected with 67% confidence\n",
      "  Position: center middle, Size: 72.2%\n",
      "\n",
      "AI DESCRIPTION: A table is a piece of furniture with a flat top and one or more legs, used as a surface for working or eating.\n",
      "\n",
      "iphone detected with 59% confidence\n",
      "  Position: right middle, Size: 10.6%\n",
      "\n",
      "AI DESCRIPTION: An iPhone is a line of smartphones designed and marketed by Apple Inc.\n",
      "\n",
      "iphone detected with 54% confidence\n",
      "  Position: right middle, Size: 14.2%\n",
      "\n",
      "AI DESCRIPTION: An iPhone is a line of smartphones designed and marketed by Apple Inc.\n",
      "\n",
      "iphone detected with 55% confidence\n",
      "  Position: right middle, Size: 13.9%\n",
      "\n",
      "iphone detected with 85% confidence\n",
      "  Position: right bottom, Size: 24.1%\n",
      "\n",
      "iphone detected with 83% confidence\n",
      "  Position: right bottom, Size: 26.1%\n",
      "\n",
      "iphone detected with 88% confidence\n",
      "  Position: right bottom, Size: 28.5%\n",
      "\n",
      "DEVICE NOTICE: Smartphone detected. Consider putting it down if not needed.\n",
      "\n",
      "iphone detected with 59% confidence\n",
      "  Position: right middle, Size: 9.3%\n",
      "\n",
      "AI DESCRIPTION: An iPhone is a line of smartphones designed and marketed by Apple Inc.\n",
      "\n",
      "iphone detected with 51% confidence\n",
      "  Position: right middle, Size: 12.3%\n",
      "\n",
      "AI DESCRIPTION: An iPhone is a line of smartphones designed and marketed by Apple Inc.\n",
      "\n",
      "iphone detected with 56% confidence\n",
      "  Position: right bottom, Size: 31.9%\n",
      "\n",
      "iphone detected with 74% confidence\n",
      "  Position: right bottom, Size: 19.2%\n",
      "\n",
      "iphone detected with 80% confidence\n",
      "  Position: right middle, Size: 27.4%\n",
      "\n",
      "AI DESCRIPTION: An iPhone is a line of smartphones designed and marketed by Apple Inc.\n",
      "\n",
      "iphone detected with 71% confidence\n",
      "  Position: right middle, Size: 39.7%\n",
      "\n",
      "iphone detected with 66% confidence\n",
      "  Position: right middle, Size: 31.5%\n",
      "\n",
      "iphone detected with 75% confidence\n",
      "  Position: right bottom, Size: 17.6%\n",
      "\n",
      "AI DESCRIPTION: An iPhone is a line of smartphones designed and marketed by Apple Inc.\n",
      "\n",
      "iphone detected with 83% confidence\n",
      "  Position: right bottom, Size: 30.4%\n",
      "\n",
      "AI DESCRIPTION: An iPhone is a line of smartphones designed and marketed by Apple Inc.\n",
      "Detection stopped.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 371\u001B[39m\n\u001B[32m    369\u001B[39m \u001B[38;5;28mprint\u001B[39m(intro)\n\u001B[32m    370\u001B[39m speak(\u001B[33m\"\u001B[39m\u001B[33mStarting YOLO object detection with voice feedback.\u001B[39m\u001B[33m\"\u001B[39m, priority=\u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m371\u001B[39m \u001B[43mrun_custom_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 322\u001B[39m, in \u001B[36mrun_custom_inference\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    319\u001B[39m     cleanup_old_objects()\n\u001B[32m    321\u001B[39m \u001B[38;5;66;03m# Perform detection\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m results = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfidence_threshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miou\u001B[49m\u001B[43m=\u001B[49m\u001B[43miou_threshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    324\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m results \u001B[38;5;129;01mand\u001B[39;00m results[\u001B[32m0\u001B[39m]:\n\u001B[32m    325\u001B[39m     annotated_frame = results[\u001B[32m0\u001B[39m].plot()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:182\u001B[39m, in \u001B[36mModel.__call__\u001B[39m\u001B[34m(self, source, stream, **kwargs)\u001B[39m\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\n\u001B[32m    154\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    155\u001B[39m     source: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28mint\u001B[39m, Image.Image, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, np.ndarray, torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    156\u001B[39m     stream: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    157\u001B[39m     **kwargs: Any,\n\u001B[32m    158\u001B[39m ) -> \u001B[38;5;28mlist\u001B[39m:\n\u001B[32m    159\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    160\u001B[39m \u001B[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001B[39;00m\n\u001B[32m    161\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    180\u001B[39m \u001B[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001B[39;00m\n\u001B[32m    181\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m182\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:552\u001B[39m, in \u001B[36mModel.predict\u001B[39m\u001B[34m(self, source, stream, predictor, **kwargs)\u001B[39m\n\u001B[32m    550\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.predictor, \u001B[33m\"\u001B[39m\u001B[33mset_prompts\u001B[39m\u001B[33m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[32m    551\u001B[39m     \u001B[38;5;28mself\u001B[39m.predictor.set_prompts(prompts)\n\u001B[32m--> \u001B[39m\u001B[32m552\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.predictor.predict_cli(source=source) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:218\u001B[39m, in \u001B[36mBasePredictor.__call__\u001B[39m\u001B[34m(self, source, model, stream, *args, **kwargs)\u001B[39m\n\u001B[32m    216\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream_inference(source, model, *args, **kwargs)\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.stream_inference(source, model, *args, **kwargs))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001B[39m, in \u001B[36m_wrap_generator.<locals>.generator_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     34\u001B[39m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[32m     35\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m         response = gen.send(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     38\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m     39\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     40\u001B[39m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:330\u001B[39m, in \u001B[36mBasePredictor.stream_inference\u001B[39m\u001B[34m(self, source, model, *args, **kwargs)\u001B[39m\n\u001B[32m    328\u001B[39m \u001B[38;5;66;03m# Inference\u001B[39;00m\n\u001B[32m    329\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m profilers[\u001B[32m1\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m330\u001B[39m     preds = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.embed:\n\u001B[32m    332\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m [preds] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(preds, torch.Tensor) \u001B[38;5;28;01melse\u001B[39;00m preds  \u001B[38;5;66;03m# yield embedding tensors\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:173\u001B[39m, in \u001B[36mBasePredictor.inference\u001B[39m\u001B[34m(self, im, *args, **kwargs)\u001B[39m\n\u001B[32m    167\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001B[39;00m\n\u001B[32m    168\u001B[39m visualize = (\n\u001B[32m    169\u001B[39m     increment_path(\u001B[38;5;28mself\u001B[39m.save_dir / Path(\u001B[38;5;28mself\u001B[39m.batch[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m]).stem, mkdir=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    170\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.visualize \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.source_type.tensor)\n\u001B[32m    171\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    172\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m173\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maugment\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43maugment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:592\u001B[39m, in \u001B[36mAutoBackend.forward\u001B[39m\u001B[34m(self, im, augment, visualize, embed, **kwargs)\u001B[39m\n\u001B[32m    590\u001B[39m \u001B[38;5;66;03m# PyTorch\u001B[39;00m\n\u001B[32m    591\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pt \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.nn_module:\n\u001B[32m--> \u001B[39m\u001B[32m592\u001B[39m     y = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maugment\u001B[49m\u001B[43m=\u001B[49m\u001B[43maugment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed\u001B[49m\u001B[43m=\u001B[49m\u001B[43membed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    594\u001B[39m \u001B[38;5;66;03m# TorchScript\u001B[39;00m\n\u001B[32m    595\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.jit:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:115\u001B[39m, in \u001B[36mBaseModel.forward\u001B[39m\u001B[34m(self, x, *args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mdict\u001B[39m):  \u001B[38;5;66;03m# for cases of training and validating while training.\u001B[39;00m\n\u001B[32m    114\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loss(x, *args, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:133\u001B[39m, in \u001B[36mBaseModel.predict\u001B[39m\u001B[34m(self, x, profile, visualize, augment, embed)\u001B[39m\n\u001B[32m    131\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m augment:\n\u001B[32m    132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._predict_augment(x)\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_predict_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:154\u001B[39m, in \u001B[36mBaseModel._predict_once\u001B[39m\u001B[34m(self, x, profile, visualize, embed)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m profile:\n\u001B[32m    153\u001B[39m     \u001B[38;5;28mself\u001B[39m._profile_one_layer(m, x, dt)\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m x = \u001B[43mm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# run\u001B[39;00m\n\u001B[32m    155\u001B[39m y.append(x \u001B[38;5;28;01mif\u001B[39;00m m.i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.save \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# save output\u001B[39;00m\n\u001B[32m    156\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m visualize:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:301\u001B[39m, in \u001B[36mC2f.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    299\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001B[39;00m\n\u001B[32m    300\u001B[39m y = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.cv1(x).chunk(\u001B[32m2\u001B[39m, \u001B[32m1\u001B[39m))\n\u001B[32m--> \u001B[39m\u001B[32m301\u001B[39m y.extend(m(y[-\u001B[32m1\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.m)\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cv2(torch.cat(y, \u001B[32m1\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:301\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    299\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001B[39;00m\n\u001B[32m    300\u001B[39m y = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.cv1(x).chunk(\u001B[32m2\u001B[39m, \u001B[32m1\u001B[39m))\n\u001B[32m--> \u001B[39m\u001B[32m301\u001B[39m y.extend(\u001B[43mm\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m[\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.m)\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cv2(torch.cat(y, \u001B[32m1\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:476\u001B[39m, in \u001B[36mBottleneck.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    474\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m    475\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m476\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x + \u001B[38;5;28mself\u001B[39m.cv2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.add \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cv2(\u001B[38;5;28mself\u001B[39m.cv1(x))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:91\u001B[39m, in \u001B[36mConv.forward_fuse\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     81\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward_fuse\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     82\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     83\u001B[39m \u001B[33;03m    Apply convolution and activation without batch normalization.\u001B[39;00m\n\u001B[32m     84\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     89\u001B[39m \u001B[33;03m        (torch.Tensor): Output tensor.\u001B[39;00m\n\u001B[32m     90\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.act(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:27:13.952188Z",
     "start_time": "2025-05-16T07:26:51.349084Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pyttsx3",
   "id": "4562add2f5296d9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.98-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting comtypes (from pyttsx3)\n",
      "  Downloading comtypes-1.4.11-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pypiwin32 (from pyttsx3)\n",
      "  Downloading pypiwin32-223-py3-none-any.whl.metadata (236 bytes)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\adity\\downloads\\yolo_room_detection\\.venv\\lib\\site-packages (from pyttsx3) (310)\n",
      "Downloading pyttsx3-2.98-py3-none-any.whl (34 kB)\n",
      "Downloading comtypes-1.4.11-py3-none-any.whl (246 kB)\n",
      "Using cached pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Installing collected packages: pypiwin32, comtypes, pyttsx3\n",
      "Successfully installed comtypes-1.4.11 pypiwin32-223 pyttsx3-2.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:23:00.336809Z",
     "start_time": "2025-05-16T05:07:48.423490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# detect_objects_cpu.py\n",
    "# YOLOv8 Custom Object Detection Script (CPU-compatible version)\n",
    "# Uses a fine-tuned YOLOv8 model for object detection on images, videos, or webcam\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def trigger_action(class_name, confidence, box_xyxy, frame_shape):\n",
    "    \"\"\"\n",
    "    Placeholder function to trigger actions based on detected objects.\n",
    "\n",
    "    Args:\n",
    "        class_name (str): Detected class name\n",
    "        confidence (float): Confidence score (0-1)\n",
    "        box_xyxy (list): Bounding box coordinates [x1, y1, x2, y2]\n",
    "        frame_shape (tuple): Frame dimensions (height, width)\n",
    "    \"\"\"\n",
    "    print(f\"Detected: '{class_name}' (Confidence: {confidence:.2f}) at Box: {box_xyxy}\")\n",
    "    # Example action logic can be added here\n",
    "\n",
    "def run_detection():\n",
    "    \"\"\"\n",
    "    Runs object detection using a custom-trained YOLOv8 model.\n",
    "    Supports images, videos, and webcam feeds with CPU fallback.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    model_path = 'yolo_room_detection_result/cpu_run12/weights/best.pt'  # Updated to match training output\n",
    "    source_input = '0'  # '0' for webcam, or path to image/video\n",
    "    confidence_threshold = 0.5  # Minimum detection confidence\n",
    "    iou_threshold = 0.45       # NMS IoU threshold\n",
    "\n",
    "    # --- Validate Paths ---\n",
    "    model_path = Path(model_path).resolve()\n",
    "    if not model_path.exists():\n",
    "        print(f\"Error: Model not found at '{model_path}'\")\n",
    "        print(f\"Current directory: {Path.cwd()}\")\n",
    "        return\n",
    "\n",
    "    # --- Load Model ---\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "        print(f\"\\nModel loaded: {model_path}\")\n",
    "        print(f\"Classes: {model.names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Hardware Setup ---\n",
    "    device = 'cpu'  # Force CPU for consistency with training\n",
    "    print(\"\\n=== Hardware Setup ===\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"(GPU available but not being used)\")\n",
    "\n",
    "    # --- Process Input Source ---\n",
    "    print(\"\\n=== Processing Setup ===\")\n",
    "    if isinstance(source_input, str) and source_input.isdigit():\n",
    "        source = int(source_input)\n",
    "        source_type = \"webcam\"\n",
    "        print(f\"Webcam source: {source}\")\n",
    "    elif isinstance(source_input, str) and Path(source_input).exists():\n",
    "        source = str(Path(source_input).resolve())\n",
    "        if Path(source_input).suffix.lower() in ['.jpg', '.png', '.jpeg']:\n",
    "            source_type = \"image\"\n",
    "            print(f\"Image source: {source}\")\n",
    "        else:\n",
    "            source_type = \"video\"\n",
    "            print(f\"Video source: {source}\")\n",
    "    else:\n",
    "        print(f\"Invalid source: {source_input}\")\n",
    "        return\n",
    "\n",
    "    # --- Run Detection ---\n",
    "    print(\"\\n=== Starting Detection ===\")\n",
    "    print(\"Press 'q' to quit during video/webcam processing\")\n",
    "\n",
    "    if source_type == \"image\":\n",
    "        try:\n",
    "            results = model(source, conf=confidence_threshold, iou=iou_threshold, device=device)\n",
    "\n",
    "            if results and results[0]:\n",
    "                annotated_frame = results[0].plot()\n",
    "                img_shape = results[0].orig_shape\n",
    "\n",
    "                for box in results[0].boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    conf = float(box.conf[0])\n",
    "                    cls_id = int(box.cls[0])\n",
    "                    class_name = model.names.get(cls_id, f\"ClassID:{cls_id}\")\n",
    "                    trigger_action(class_name, conf, [x1, y1, x2, y2], img_shape)\n",
    "\n",
    "                cv2.imshow(\"YOLOv8 Detection\", annotated_frame)\n",
    "                cv2.waitKey(0)\n",
    "            else:\n",
    "                print(\"No detections found\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Image processing error: {e}\")\n",
    "        finally:\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    else:  # Video or webcam\n",
    "        cap = cv2.VideoCapture(source)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open {source_type} source\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                results = model(frame, conf=confidence_threshold, iou=iou_threshold, device=device, verbose=False)\n",
    "\n",
    "                if results and results[0]:\n",
    "                    annotated_frame = results[0].plot()\n",
    "                    frame_shape = results[0].orig_shape\n",
    "\n",
    "                    for box in results[0].boxes:\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        conf = float(box.conf[0\n",
    "                                     ])\n",
    "                        cls_id = int(box.cls[0])\n",
    "                        class_name = model.names.get(cls_id, f\"ClassID:{cls_id}\")\n",
    "                        trigger_action(class_name, conf, [x1, y1, x2, y2], frame_shape)\n",
    "\n",
    "                    cv2.imshow(\"YOLOv8 Detection\", annotated_frame)\n",
    "                else:\n",
    "                    cv2.imshow(\"YOLOv8 Detection\", frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{source_type} processing error: {e}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    print(\"\\n=== Detection Complete ===\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"=== YOLOv8 CPU Detection ===\")\n",
    "    print(\"Note: This script uses CPU by default\")\n",
    "    print(\"Configure model_path and source_input as needed\")\n",
    "    print(\"----------------------------\")\n",
    "    run_detection()"
   ],
   "id": "c5caf8317d98832e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YOLOv8 CPU Detection ===\n",
      "Note: This script uses CPU by default\n",
      "Configure model_path and source_input as needed\n",
      "----------------------------\n",
      "\n",
      "Model loaded: C:\\Users\\adity\\Downloads\\yolo_room_detection\\yolo_room_detection_result\\cpu_run12\\weights\\best.pt\n",
      "Classes: {0: 'backpack', 1: 'bottle-a', 2: 'bottle-b', 3: 'bowl', 4: 'casserole', 5: 'chair', 6: 'cup', 7: 'fork', 8: 'frigo', 9: 'glass', 10: 'handbag', 11: 'iphone', 12: 'knife', 13: 'lamp', 14: 'laptop', 15: 'macbook', 16: 'micro-ondes', 17: 'oldphone', 18: 'paperbag', 19: 'plate', 20: 'smartphone', 21: 'sofa', 22: 'spoon', 23: 'table', 24: 'washmachine'}\n",
      "\n",
      "=== Hardware Setup ===\n",
      "Using device: cpu\n",
      "\n",
      "=== Processing Setup ===\n",
      "Webcam source: 0\n",
      "\n",
      "=== Starting Detection ===\n",
      "Press 'q' to quit during video/webcam processing\n",
      "Detected: 'iphone' (Confidence: 0.72) at Box: [322, 179, 639, 479]\n",
      "Detected: 'iphone' (Confidence: 0.75) at Box: [334, 185, 639, 480]\n",
      "Detected: 'macbook' (Confidence: 0.50) at Box: [0, 0, 637, 480]\n",
      "Detected: 'macbook' (Confidence: 0.56) at Box: [1, 0, 637, 480]\n",
      "Detected: 'macbook' (Confidence: 0.63) at Box: [1, 0, 636, 480]\n",
      "Detected: 'macbook' (Confidence: 0.67) at Box: [0, 0, 637, 480]\n",
      "Detected: 'iphone' (Confidence: 0.83) at Box: [126, 196, 579, 479]\n",
      "Detected: 'iphone' (Confidence: 0.64) at Box: [127, 182, 636, 479]\n",
      "Detected: 'iphone' (Confidence: 0.75) at Box: [123, 195, 639, 480]\n",
      "Detected: 'iphone' (Confidence: 0.66) at Box: [98, 200, 625, 480]\n",
      "Detected: 'iphone' (Confidence: 0.62) at Box: [96, 197, 618, 480]\n",
      "Detected: 'iphone' (Confidence: 0.72) at Box: [226, 227, 586, 480]\n",
      "Detected: 'iphone' (Confidence: 0.63) at Box: [151, 229, 604, 480]\n",
      "Detected: 'iphone' (Confidence: 0.72) at Box: [134, 227, 633, 480]\n",
      "Detected: 'iphone' (Confidence: 0.71) at Box: [178, 233, 634, 480]\n",
      "Detected: 'iphone' (Confidence: 0.81) at Box: [131, 196, 628, 480]\n",
      "Detected: 'iphone' (Confidence: 0.60) at Box: [93, 359, 507, 480]\n",
      "Detected: 'iphone' (Confidence: 0.64) at Box: [92, 356, 516, 480]\n",
      "Detected: 'iphone' (Confidence: 0.63) at Box: [90, 358, 504, 480]\n",
      "Detected: 'iphone' (Confidence: 0.68) at Box: [104, 360, 502, 479]\n",
      "Detected: 'iphone' (Confidence: 0.57) at Box: [131, 370, 489, 480]\n",
      "Detected: 'iphone' (Confidence: 0.52) at Box: [137, 374, 485, 480]\n",
      "Detected: 'iphone' (Confidence: 0.54) at Box: [109, 355, 534, 479]\n",
      "Detected: 'iphone' (Confidence: 0.76) at Box: [81, 283, 639, 480]\n",
      "Detected: 'iphone' (Confidence: 0.80) at Box: [171, 288, 640, 480]\n",
      "Detected: 'iphone' (Confidence: 0.64) at Box: [124, 313, 560, 480]\n",
      "Detected: 'iphone' (Confidence: 0.63) at Box: [158, 286, 638, 480]\n",
      "Detected: 'iphone' (Confidence: 0.82) at Box: [1, 253, 488, 480]\n",
      "Detected: 'iphone' (Confidence: 0.79) at Box: [4, 271, 526, 479]\n",
      "Detected: 'sofa' (Confidence: 0.51) at Box: [4, 280, 596, 479]\n",
      "Detected: 'iphone' (Confidence: 0.67) at Box: [6, 277, 620, 480]\n",
      "Detected: 'iphone' (Confidence: 0.53) at Box: [2, 289, 540, 479]\n",
      "Detected: 'iphone' (Confidence: 0.56) at Box: [0, 274, 548, 479]\n",
      "Detected: 'macbook' (Confidence: 0.70) at Box: [1, 0, 640, 480]\n",
      "Detected: 'macbook' (Confidence: 0.71) at Box: [2, 0, 640, 480]\n",
      "Detected: 'macbook' (Confidence: 0.53) at Box: [2, 0, 640, 480]\n",
      "Detected: 'chair' (Confidence: 0.72) at Box: [3, 0, 640, 480]\n",
      "Detected: 'frigo' (Confidence: 0.69) at Box: [0, 1, 477, 480]\n",
      "Detected: 'frigo' (Confidence: 0.61) at Box: [4, 0, 531, 480]\n",
      "Detected: 'macbook' (Confidence: 0.54) at Box: [4, 1, 639, 480]\n",
      "Detected: 'frigo' (Confidence: 0.84) at Box: [0, 0, 637, 480]\n",
      "Detected: 'macbook' (Confidence: 0.60) at Box: [5, 0, 637, 480]\n",
      "Detected: 'macbook' (Confidence: 0.50) at Box: [6, 0, 638, 480]\n",
      "Detected: 'macbook' (Confidence: 0.65) at Box: [35, 0, 638, 480]\n",
      "Detected: 'iphone' (Confidence: 0.63) at Box: [139, 290, 515, 480]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 149\u001B[39m\n\u001B[32m    147\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mConfigure model_path and source_input as needed\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    148\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m----------------------------\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m149\u001B[39m \u001B[43mrun_detection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 116\u001B[39m, in \u001B[36mrun_detection\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[32m    114\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m results = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfidence_threshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miou\u001B[49m\u001B[43m=\u001B[49m\u001B[43miou_threshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m results \u001B[38;5;129;01mand\u001B[39;00m results[\u001B[32m0\u001B[39m]:\n\u001B[32m    119\u001B[39m     annotated_frame = results[\u001B[32m0\u001B[39m].plot()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:182\u001B[39m, in \u001B[36mModel.__call__\u001B[39m\u001B[34m(self, source, stream, **kwargs)\u001B[39m\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\n\u001B[32m    154\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    155\u001B[39m     source: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28mint\u001B[39m, Image.Image, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, np.ndarray, torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    156\u001B[39m     stream: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    157\u001B[39m     **kwargs: Any,\n\u001B[32m    158\u001B[39m ) -> \u001B[38;5;28mlist\u001B[39m:\n\u001B[32m    159\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    160\u001B[39m \u001B[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001B[39;00m\n\u001B[32m    161\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    180\u001B[39m \u001B[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001B[39;00m\n\u001B[32m    181\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m182\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:552\u001B[39m, in \u001B[36mModel.predict\u001B[39m\u001B[34m(self, source, stream, predictor, **kwargs)\u001B[39m\n\u001B[32m    550\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.predictor, \u001B[33m\"\u001B[39m\u001B[33mset_prompts\u001B[39m\u001B[33m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[32m    551\u001B[39m     \u001B[38;5;28mself\u001B[39m.predictor.set_prompts(prompts)\n\u001B[32m--> \u001B[39m\u001B[32m552\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.predictor.predict_cli(source=source) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:218\u001B[39m, in \u001B[36mBasePredictor.__call__\u001B[39m\u001B[34m(self, source, model, stream, *args, **kwargs)\u001B[39m\n\u001B[32m    216\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream_inference(source, model, *args, **kwargs)\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.stream_inference(source, model, *args, **kwargs))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001B[39m, in \u001B[36m_wrap_generator.<locals>.generator_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     34\u001B[39m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[32m     35\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m         response = gen.send(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     38\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m     39\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     40\u001B[39m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:330\u001B[39m, in \u001B[36mBasePredictor.stream_inference\u001B[39m\u001B[34m(self, source, model, *args, **kwargs)\u001B[39m\n\u001B[32m    328\u001B[39m \u001B[38;5;66;03m# Inference\u001B[39;00m\n\u001B[32m    329\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m profilers[\u001B[32m1\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m330\u001B[39m     preds = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.embed:\n\u001B[32m    332\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m [preds] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(preds, torch.Tensor) \u001B[38;5;28;01melse\u001B[39;00m preds  \u001B[38;5;66;03m# yield embedding tensors\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:173\u001B[39m, in \u001B[36mBasePredictor.inference\u001B[39m\u001B[34m(self, im, *args, **kwargs)\u001B[39m\n\u001B[32m    167\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001B[39;00m\n\u001B[32m    168\u001B[39m visualize = (\n\u001B[32m    169\u001B[39m     increment_path(\u001B[38;5;28mself\u001B[39m.save_dir / Path(\u001B[38;5;28mself\u001B[39m.batch[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m]).stem, mkdir=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    170\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.visualize \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.source_type.tensor)\n\u001B[32m    171\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    172\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m173\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maugment\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43maugment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:592\u001B[39m, in \u001B[36mAutoBackend.forward\u001B[39m\u001B[34m(self, im, augment, visualize, embed, **kwargs)\u001B[39m\n\u001B[32m    590\u001B[39m \u001B[38;5;66;03m# PyTorch\u001B[39;00m\n\u001B[32m    591\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pt \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.nn_module:\n\u001B[32m--> \u001B[39m\u001B[32m592\u001B[39m     y = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maugment\u001B[49m\u001B[43m=\u001B[49m\u001B[43maugment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed\u001B[49m\u001B[43m=\u001B[49m\u001B[43membed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    594\u001B[39m \u001B[38;5;66;03m# TorchScript\u001B[39;00m\n\u001B[32m    595\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.jit:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:115\u001B[39m, in \u001B[36mBaseModel.forward\u001B[39m\u001B[34m(self, x, *args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mdict\u001B[39m):  \u001B[38;5;66;03m# for cases of training and validating while training.\u001B[39;00m\n\u001B[32m    114\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loss(x, *args, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:133\u001B[39m, in \u001B[36mBaseModel.predict\u001B[39m\u001B[34m(self, x, profile, visualize, augment, embed)\u001B[39m\n\u001B[32m    131\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m augment:\n\u001B[32m    132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._predict_augment(x)\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_predict_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:154\u001B[39m, in \u001B[36mBaseModel._predict_once\u001B[39m\u001B[34m(self, x, profile, visualize, embed)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m profile:\n\u001B[32m    153\u001B[39m     \u001B[38;5;28mself\u001B[39m._profile_one_layer(m, x, dt)\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m x = \u001B[43mm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# run\u001B[39;00m\n\u001B[32m    155\u001B[39m y.append(x \u001B[38;5;28;01mif\u001B[39;00m m.i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.save \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# save output\u001B[39;00m\n\u001B[32m    156\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m visualize:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:224\u001B[39m, in \u001B[36mSPPF.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    222\u001B[39m y = [\u001B[38;5;28mself\u001B[39m.cv1(x)]\n\u001B[32m    223\u001B[39m y.extend(\u001B[38;5;28mself\u001B[39m.m(y[-\u001B[32m1\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m3\u001B[39m))\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcv2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:91\u001B[39m, in \u001B[36mConv.forward_fuse\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     81\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward_fuse\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     82\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     83\u001B[39m \u001B[33;03m    Apply convolution and activation without batch normalization.\u001B[39;00m\n\u001B[32m     84\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     89\u001B[39m \u001B[33;03m        (torch.Tensor): Output tensor.\u001B[39;00m\n\u001B[32m     90\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:432\u001B[39m, in \u001B[36mSiLU.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    431\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m432\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43msilu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\yolo_room_detection\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2379\u001B[39m, in \u001B[36msilu\u001B[39m\u001B[34m(input, inplace)\u001B[39m\n\u001B[32m   2377\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(silu, (\u001B[38;5;28minput\u001B[39m,), \u001B[38;5;28minput\u001B[39m, inplace=inplace)\n\u001B[32m   2378\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[32m-> \u001B[39m\u001B[32m2379\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_nn\u001B[49m\u001B[43m.\u001B[49m\u001B[43msilu_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   2380\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m torch._C._nn.silu(\u001B[38;5;28minput\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
